{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting, analyzing and visualizing spatial entities from Tripadvisor text data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries and Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import os\n",
    "import sys\n",
    "import spacy\n",
    "import requests\n",
    "import folium\n",
    "from folium import Popup\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from wordcloud import WordCloud\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data about day trips in Switzerland\n",
    "\n",
    "Note that the data were collected using the Chrome Web Scraper extension (see file 'webscraper_tripadvisor.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data from Tripadvisor derived via web scraping\n",
    "df = pd.read_csv('tripadvisor.csv')\n",
    "\n",
    "# Get the shape of the dataframe\n",
    "print(df.shape)\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract locations by using Named Entity Recognition (NER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show locations in titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All titles\n",
    "print(df['title_raw'])\n",
    "\n",
    "# Single title\n",
    "print('\\n', df.loc[df['web-scraper-order'] == '1713267369-7']['title_raw'].values[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot wordcloud from 'title_raw'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join all reviews into a single string and create a WordCloud object\n",
    "text = ' '.join(review for review in df['title_raw'])\n",
    "\n",
    "# Create and generate a word cloud image:\n",
    "wordcloud = WordCloud(background_color=\"black\").generate(text)\n",
    "\n",
    "# Display the generated image:\n",
    "plt.figure(figsize=(7, 6))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entity Recognition (NER) example\n",
    "\n",
    "Note that, when you use a language model in SpaCy, like \"en_core_web_sm\", it processes the text to perform various Natural Language Processing (NLP) tasks. These tasks typically include tokenization, part-of-speech tagging, dependency parsing, lemmatization, and named entity recognition (NER).\n",
    "\n",
    "For details see: https://spacy.io/api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the German language model\n",
    "nlp = spacy.load(\"de_core_news_sm\")\n",
    "\n",
    "# Example texts with their identifiers\n",
    "df_example = pd.DataFrame({\n",
    "                    'id': ['1', '2', '3'],\n",
    "                    'title_raw': ['Titlis-Tagesausflug mit privatem Reiseleiter ab ZÃ¼rich.', \n",
    "                                  'VIP-Erlebnis zum Comer See und nach Lugano.',\n",
    "                                  'Private Schneewanderung im Ab Berner Oberland.']})\n",
    "\n",
    "texts = df_example['title_raw']\n",
    "\n",
    "# Loop through the texts and extract the entities\n",
    "for text in texts:\n",
    "    doc = nlp(text)\n",
    "    for ent in doc.ents:\n",
    "        print(ent.text, '|', ent.label_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract locations from Tripadvisor data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the German language model\n",
    "nlp = spacy.load(\"de_core_news_sm\")\n",
    "\n",
    "# Example texts with their identifiers\n",
    "texts = df[['web-scraper-order', 'title_raw']]\n",
    "\n",
    "# Function to extract locations, their labels, and web-scraper-order from original data\n",
    "def extract_locations(texts):\n",
    "    locations = []\n",
    "    for idx, row in texts.iterrows():\n",
    "        doc = nlp(row['title_raw'])\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ == \"LOC\" or ent.label_ == \"GPE\":\n",
    "                locations.append((row['web-scraper-order'], ent.text, ent.label_))\n",
    "    return locations\n",
    "\n",
    "# Extract locations\n",
    "locations_with_labels_ids = extract_locations(texts)\n",
    "\n",
    "# Create a DataFrame with location names, their labels, and the IDs\n",
    "df_locations = pd.DataFrame(locations_with_labels_ids, columns=['web-scraper-order', 'location', 'entity_type'])\n",
    "\n",
    "# Remove duplicate locations\n",
    "df_locations_unique = df_locations.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# Filter out location names longer than 25 characters\n",
    "df_locations_filtered = df_locations_unique[df_locations_unique['location'].apply(len) <= 25]\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df_locations_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geocode locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the base URL for the GeoAdmin API\n",
    "base_url = \"https://api3.geo.admin.ch/rest/services/api/SearchServer?\"\n",
    "\n",
    "# Initialize a list to store IDs, coordinates, or NA along with location names\n",
    "coordinates_list = []\n",
    "\n",
    "# Loop through each address and ID in the DataFrame (with progress bar)\n",
    "for idx, row in tqdm(df_locations_filtered.iterrows(),\n",
    "                     total = df_locations_filtered.shape[0], \n",
    "                     bar_format='{l_bar}{bar}'):\n",
    "    \n",
    "    # Set up search parameters for each location\n",
    "    parameters = {\n",
    "        \"searchText\": row['location'],\n",
    "        \"origins\": \"address\",\n",
    "        \"type\": \"locations\"\n",
    "    }\n",
    "\n",
    "    # Send the request to the GeoAdmin API\n",
    "    response = requests.get(base_url, params=parameters)\n",
    "\n",
    "    # Check the response status and process the data\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        if data['results']:\n",
    "            first_result = data['results'][0]['attrs']\n",
    "            lat = first_result.get('lat', None)\n",
    "            lon = first_result.get('lon', None)\n",
    "            coordinates_list.append([row['web-scraper-order'], \n",
    "                                     row['location'], lat, lon])\n",
    "        else:\n",
    "            coordinates_list.append([row['web-scraper-order'], \n",
    "                                     row['location'], None, None])\n",
    "    else:\n",
    "        coordinates_list.append([row['web-scraper-order'], \n",
    "                                 row['location'], None, None])\n",
    "        \n",
    "# Convert the list to a DataFrame with specified column names\n",
    "df_geocoded = pd.DataFrame(coordinates_list, columns=['web-scraper-order', \n",
    "                                                      'location', \n",
    "                                                      'latitude', \n",
    "                                                      'longitude'])\n",
    "\n",
    "# Display the DataFrame\n",
    "df_geocoded\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge the original DataFrame with the geocoded DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the original DataFrame with the geocoded DataFrame\n",
    "merged_df_orig = pd.merge(df[['web-scraper-order', \n",
    "                              'title_raw', \n",
    "                              'price_raw', \n",
    "                              'ranking_raw', \n",
    "                              'duration_raw',\n",
    "                              'text_raw']], df_geocoded, \n",
    "                              on='web-scraper-order', \n",
    "                              how='left')\n",
    "merged_df_orig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter day trips with price information 'per adult'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check price 'per adult'\n",
    "merged_df_orig['price_per_adult'] = merged_df_orig['text_raw'].str.contains('pro Erwachsenem').astype(int)\n",
    "\n",
    "# Filter only prices with information 'per adult'\n",
    "merged_df = merged_df_orig[merged_df_orig['price_per_adult'] == 1].reset_index(drop=True)\n",
    "\n",
    "# Show remaining number of rows\n",
    "print(merged_df.shape)\n",
    "\n",
    "# Display the DataFrame\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze prices of day trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract numbers from the string\n",
    "merged_df['price'] = merged_df['price_raw'].str.strip()\n",
    "merged_df['price'] = merged_df['price'].str.extract(r'(\\d+â?\\d*)')\n",
    "merged_df['price'] = merged_df['price'].str.replace(\"â\", \"\").astype(int)\n",
    "\n",
    "# Pivot table to get the average price for each location\n",
    "pivot_table = merged_df.pivot_table(index='location', \n",
    "                                    values='price', \n",
    "                                    aggfunc='mean').reset_index()\n",
    "\n",
    "pivot_table.sort_values(by='price', ascending=False)\n",
    "\n",
    "# Histogram with average price of trips\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.hist(pivot_table['price'])\n",
    "plt.title('Avg. price of trips')\n",
    "plt.xlabel('Price per adult (CHF)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Filter most expensive trips\n",
    "most_expensive_trips = merged_df[merged_df['price'] >= 1500].sort_values(by='price', ascending=False)\n",
    "most_expensive_trips[['web-scraper-order', 'title_raw', 'price', 'location', 'duration_raw', 'ranking_raw']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze duration of day trips\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract numbers from the string\n",
    "merged_df['max_duration'] = merged_df['duration_raw'].str.strip()\n",
    "merged_df['max_duration'] = merged_df['max_duration'].str.extractall(r'(\\d+)').astype(int).groupby(level=0).max()\n",
    "\n",
    "# Pivot table to get the average duration for each trip\n",
    "pivot_table = merged_df.pivot_table(index='location', \n",
    "                                    values='max_duration', \n",
    "                                    aggfunc='mean').reset_index()\n",
    "\n",
    "sorted_pivot = pivot_table.sort_values(by='max_duration', ascending=False)\n",
    "\n",
    "# Histogram with average max. duration of trips\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.hist(pivot_table['max_duration'])\n",
    "plt.title('Avg. maximum duration of day trips')\n",
    "plt.xlabel('Avg. maximum duration (hours)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xlim(0, 12)\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse rankings of day trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract numbers from the string\n",
    "merged_df['ranking'] = merged_df['ranking_raw'].str.strip()\n",
    "merged_df['ranking'] = merged_df['ranking'].str[:3].astype(float)\n",
    "\n",
    "# Pivot table to get the average ranking for each trip\n",
    "pivot_table = merged_df.pivot_table(index='location', \n",
    "                                    values='ranking', \n",
    "                                    aggfunc='mean').reset_index()\n",
    "\n",
    "pivot_table.sort_values(by='ranking', ascending=False)\n",
    "\n",
    "# Histogram with avg. ranking of trips\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.hist(pivot_table['ranking'])\n",
    "plt.title('Avg. ranking of day trips')\n",
    "plt.xlabel('Ranking (1 = worst ... 5 = best)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xlim(1, 5)\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "merged_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot locations on map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisierung der Map\n",
    "m = folium.Map(location=[47.44, 8.65], zoom_start=8)\n",
    "\n",
    "# Add lat/lon of addresses\n",
    "df_sub = merged_df.dropna().drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "for i in range(0, len(df_sub)):\n",
    "    popup_text = (\n",
    "        f\"Location: {df_sub.iloc[i]['location']}, \"\n",
    "        f\"Ranking: {df_sub.iloc[i]['ranking']}, \"\n",
    "        f\"Price per adult: {df_sub.iloc[i]['price']}, \"\n",
    "        f\"Duration in hours: {df_sub.iloc[i]['max_duration']}\"\n",
    "    )\n",
    "    popup = folium.Popup(popup_text, max_width=500)\n",
    "    folium.Marker(location=(df_sub.iloc[i]['latitude'], df_sub.iloc[i]['longitude']), popup=popup).add_to(m)\n",
    "    \n",
    "# Layer control\n",
    "folium.LayerControl().add_to(m)\n",
    "\n",
    "# Plot map\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jupyter notebook --footer info-- (please always provide this at the end of each notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import platform\n",
    "import socket\n",
    "from platform import python_version\n",
    "from datetime import datetime\n",
    "\n",
    "print('-----------------------------------')\n",
    "print(os.name.upper())\n",
    "print(platform.system(), '|', platform.release())\n",
    "print('Datetime:', datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "print('Python Version:', python_version())\n",
    "print('-----------------------------------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gisenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
